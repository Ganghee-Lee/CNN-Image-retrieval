{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ranking_Module.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1mOv2k-CxD-2XXbLWWiNw7v2zlkwThY_x","authorship_tag":"ABX9TyPsgnoxYtuCJ2CMv4Do8Np2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lc4LRiG5w3D_","colab_type":"text"},"source":["# Import Libs"]},{"cell_type":"code","metadata":{"id":"hxLpgloyzO1S","colab_type":"code","colab":{}},"source":["from scipy import spatial\n","import os\n","import pickle\n","from six.moves import cPickle\n","from keras import backend as K\n","from keras.layers import Input, Conv2D, MaxPooling2D\n","from keras.models import Model\n","import cv2\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import operator"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ReYlUP-nxFCG","colab_type":"text"},"source":["# Path"]},{"cell_type":"code","metadata":{"id":"SDvAByXCw5tg","colab_type":"code","colab":{}},"source":["base_path = 'drive/My Drive/capstone/'\n","\n","#test_path = 'drive/My Drive/capstone/retrieval_test/retrieval_annotations.txt' # Test data (annotation file)\n","\n","test_base_path = 'drive/My Drive/capstone/retrieval_test' # Directory to save the test images\n","\n","config_output_filename = os.path.join(base_path, 'model_vgg_config.pickle')\n","\n","pickles = 'backbone_pickles'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5HsPCe6xMGL","colab_type":"text"},"source":["# Config Setting\n"]},{"cell_type":"code","metadata":{"id":"k_rBO-f6Qqd1","colab_type":"code","colab":{}},"source":["class Config:\n","\n","\tdef __init__(self):\n","\n","\t\t# Print the process or not\n","\t\tself.verbose = True\n","\n","\t\t# base network --> Todo! ResNet 으로 바꿔야함\n","\t\tself.network = 'vgg'\n","\n","\t\t# Setting for data augmentation\n","\t\tself.use_horizontal_flips = False\n","\t\tself.use_vertical_flips = False\n","\t\tself.rot_90 = False\n","\n","\t\t# Anchor box scales\n","    \t# If im_size is smaller, anchor_box_scales should be scaled\n","\t    # Original anchor_box_scales in the paper is [128, 256, 512]\n","\t\tself.anchor_box_scales = [64, 128, 256] \n","\n","\t\t# Anchor box ratios\n","\t\tself.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n","\n","\t\t# Size to resize the smallest side of the image\n","\t\t# Original setting in paper is 600. Set to 300 in here to save training time\n","\t\tself.im_size = 300\n","\n","\t\t# image channel-wise mean to subtract\n","\t\tself.img_channel_mean = [103.939, 116.779, 123.68]\n","\t\tself.img_scaling_factor = 1.0\n","\n","\t\t# number of ROIs at once --> Todo! If ResNet, RoI will be 3.\n","\t\tself.num_rois = 4\n","\n","\t\t# stride at the RPN (this depends on the network configuration)\n","\t\tself.rpn_stride = 16\n","\n","\t\tself.balanced_classes = False\n","\n","\t\t# scaling the stdev\n","\t\tself.std_scaling = 4.0\n","\t\tself.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n","\n","\t\t# overlaps for RPN --> Todo! If ResNet, IoU can be reduced. (what value? : 아마 0.5도 괜찮을듯 논문에서 본 적 있음)\n","\t\tself.rpn_min_overlap = 0.3\n","\t\tself.rpn_max_overlap = 0.7\n","\n","\t\t# overlaps for classifier ROIs\n","\t\tself.classifier_min_overlap = 0.1\n","\t\tself.classifier_max_overlap = 0.5\n","\n","\t\t# placeholder for the class mapping, automatically generated by the parser\n","\t\tself.class_mapping = None\n","\n","\t\tself.model_path = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WRoF2ww-xWj2","colab_type":"text"},"source":["# VGG Model"]},{"cell_type":"code","metadata":{"id":"7fs0CZLZPPbU","colab_type":"code","colab":{}},"source":["def get_img_output_length(width, height):\n","    def get_output_length(input_length):\n","        return input_length//16\n","\n","    return get_output_length(width), get_output_length(height)    \n","\n","def nn_base(input_tensor=None, trainable=False):\n","\n","\n","    input_shape = (None, None, 3)\n","\n","    if input_tensor is None:\n","        img_input = Input(shape=input_shape)\n","    else:\n","        if not K.is_keras_tensor(input_tensor):\n","            img_input = Input(tensor=input_tensor, shape=input_shape)\n","        else:\n","            img_input = input_tensor\n","\n","    bn_axis = 3\n","\n","    # Block 1\n","    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n","    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n","\n","    # Block 2\n","    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n","    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n","\n","    # Block 3\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n","\n","    # Block 4\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n","\n","    # Block 5\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n","    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BJA-6VyZxekt","colab_type":"text"},"source":["# Load Config Pickle"]},{"cell_type":"code","metadata":{"id":"YKvzZ4ixOjZm","colab_type":"code","colab":{}},"source":["\n","with open(config_output_filename, 'rb') as f_in:\n","\tC = pickle.load(f_in)\n","\n","# turn off any data augmentation at test time\n","C.use_horizontal_flips = False\n","C.use_vertical_flips = False\n","C.rot_90 = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ezsgPy5OJi9","colab_type":"text"},"source":["# Image Preprocessing"]},{"cell_type":"code","metadata":{"id":"D1VR6fSPOI3j","colab_type":"code","colab":{}},"source":["def format_img_size(img, C):\n","\t\"\"\" formats the image size based on config \"\"\"\n","\timg_min_side = float(C.im_size)\n","\t(height,width,_) = img.shape\n","\t\t\n","\tif width <= height:\n","\t\tratio = img_min_side/width\n","\t\tnew_height = int(ratio * height)\n","\t\tnew_width = int(img_min_side)\n","\telse:\n","\t\tratio = img_min_side/height\n","\t\tnew_width = int(ratio * width)\n","\t\tnew_height = int(img_min_side)\n","\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n","\treturn img, ratio\t\n","\n","def format_img_channels(img, C):\n","\t\"\"\" formats the image channels based on config \"\"\"\n","\timg = img[:, :, (2, 1, 0)]\n","\timg = img.astype(np.float32)\n","\timg[:, :, 0] -= C.img_channel_mean[0]\n","\timg[:, :, 1] -= C.img_channel_mean[1]\n","\timg[:, :, 2] -= C.img_channel_mean[2]\n","\timg /= C.img_scaling_factor\n","\timg = np.transpose(img, (2, 0, 1))\n","\timg = np.expand_dims(img, axis=0)\n","\treturn img\n","\n","def format_img(img, C):\n","\t\"\"\" formats an image for model prediction based on config \"\"\"\n","\timg, ratio = format_img_size(img, C)\n","\timg = format_img_channels(img, C)\n","\treturn img, ratio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IFWbZQ2_xl1R","colab_type":"text"},"source":["# Construct and Load the Model"]},{"cell_type":"code","metadata":{"id":"x4GGU3up0c2T","colab_type":"code","colab":{}},"source":["num_features = 512\n","\n","input_shape_img = (None, None, 3)\n","\n","img_input = Input(shape=input_shape_img)\n","\n","\n","# define the base network *Todo : (VGG here, can be Resnet50 ...)*\n","shared_layers = nn_base(img_input, trainable=True)\n","\n","# model backbone\n","model_backbone = Model(img_input, shared_layers)\n","model_backbone.load_weights(C.model_path, by_name=True)\n","# model backbone"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2kMs3oyyx2ED","colab_type":"text"},"source":["# Load pickles\n"," - Extracted features from each images using max pooling\n"," - image represented by 512 values"]},{"cell_type":"code","metadata":{"id":"pmDi1Zlnx1rO","colab_type":"code","colab":{}},"source":["'''\n","Load Pickles \n"," - predict 512 values using backbone model\n"," - saved these values as pickle file\n","'''\n","load_pickles = cPickle.load(open(os.path.join(test_base_path,pickles), \"rb\", True))\n","print(len(load_pickles.keys()))\n","print(load_pickles.items())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRk_EJH2yisa","colab_type":"text"},"source":["# Demo\n"," - Tested ranking module by query image"]},{"cell_type":"code","metadata":{"id":"-q8z6c8Qz6mY","colab_type":"code","colab":{}},"source":["print(load_pickles.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"frMZg6U4Bf-F","colab_type":"text"},"source":["# Version 1 : Input is image path & exists in database"]},{"cell_type":"code","metadata":{"id":"9donid1vyiIZ","colab_type":"code","colab":{}},"source":["default = 'drive/My Drive/capstone/retrieval_test/tiger_46.jpg'\n","def ranking_module(query=default) :\n","    #query='drive/My Drive/capstone/retrieval_test/tiger_46.jpg'\n","    if query in load_pickles.keys() :\n","        query_pooling=load_pickles[query]['avg_pooling']\n","        query_class=load_pickles[query]['cls']\n","        \n","    else :\n","        X, ratio = format_img(query, C)\n","    \n","        X = np.transpose(X, (0, 2, 3, 1))\n","        \n","        # 변형부분\n","        result=model_backbone.predict(X)\n","        result=model_backbone.predict(X)\n","\n","        classname=pickle_img[filepath]\n","        query_pooling = np.max(result[0], axis=(0, 1))\n","\n","    \n","    result={}\n","    for filepath, value in load_pickles.items() :\n","        cosine=spatial.distance.cosine(query_pooling, value['avg_pooling'])\n","        if query_class and query_class != value['cls'] :\n","            cosine+=1\n","        result[filepath] = cosine\n","\n","    result=sorted(result.items(), key=lambda x: x[1])\n","    k=0\n","    img = cv2.imread(query)\n","    plt.figure(figsize=(24,20))\n","    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n","    plt.subplot(653),plt.imshow(cv2.cvtColor(cv2.imread(query), cv2.COLOR_BGR2RGB)),plt.title('query image')\n","    for i in result[:-20] :\n","        print(i[0])\n","        print(i[1])\n","        img = cv2.imread(i[0])\n","\n","        plt.subplot(6,5,k+6),plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","        k+=1\n","ranking_module()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwIv68R9ICP2","colab_type":"text"},"source":["# Version 2 : Input is pixel data & not exists in database"]},{"cell_type":"code","metadata":{"id":"fe-C222P7vvO","colab_type":"code","colab":{}},"source":["from PIL import Image\n","def ranking_module(query) :\n","    query_class=''\n","\n","    X, ratio = format_img(query, C)\n","\n","    X = np.transpose(X, (0, 2, 3, 1))\n","    \n","    # 변형부분\n","    result=model_backbone.predict(X)\n","\n","    query_pooling = np.max(result[0], axis=(0, 1))\n","\n","    result={}\n","    \n","    for filepath, value in load_pickles.items() :\n","        cosine=spatial.distance.cosine(query_pooling, value['avg_pooling'])\n","        result[filepath] = [cosine, value['id']]\n","        \n","\n","    cls_count={'rose' : 0, 'tiger' : 0, 'dragon' : 0, 'geometric' : 0}\n","    temp_result=sorted(result.items(), key=lambda x: x[1][0])\n","    \n","    for i in temp_result[:20] :\n","        cls_count[load_pickles[i[0]]['cls']]+=1\n","    # max_key = max(cls_count.items(), key=lambda k: k[1])\n","    print(cls_count)\n","    query_class=sorted(cls_count.items(), key=lambda x: x[1], reverse=True)\n","    print(query_class)\n","    for filename, value in result.items() :\n","        alpha = 1 if cls_count[load_pickles[filename]['cls']]==0 else cls_count[load_pickles[filename]['cls']]\n","        result[filename][0] +=(20/alpha) * 0.08\n","        \n","        # if load_pickles[filename]['cls'] == query_class[0][0] :\n","\n","        #     continue\n","        # elif load_pickles[filename]['cls'] == query_class[1][0] :\n","        #     result[filename][0] *= 2\n","        #     continue\n","        # else : \n","        #     result[filename][0] *= 2.5\n","        #     continue\n","    \n","    result=sorted(result.items(), key=lambda x: x[1])\n","    #print(query_class)\n","    print(result)\n","    \n","\n","\n","    img=query\n","    plt.figure(figsize=(24,20))\n","    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n","    plt.subplot(653),plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)),plt.title('query image')\n","    frame   = plt.gca()\n","    frame.axes.get_xaxis().set_visible(False)\n","    frame.axes.get_yaxis().set_visible(False)\n","    k=0\n","    for i in result :\n","        print(i[1][1])\n","        # print(i[1][0])\n","    result = result[1:3] + result[4:9] + result[10:]\n","    for i in result[:20] :\n","        \n","        img = cv2.imread(i[0])\n","        \n","        #plt.subplot(6,5,k+6),plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)), plt.title(\"rank \" + str(k+1) + \" : \" + str(i[1][0]))\n","        plt.subplot(6,5,k+6),plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)), plt.title(\"rank \" + str(k+1))\n","        k+=1\n","        frame   = plt.gca()\n","        frame.axes.get_xaxis().set_visible(False)\n","        frame.axes.get_yaxis().set_visible(False)\n","    id_output=[x[1][1] for x in result]\n","\n","    return id_output\n","im='drive/My Drive/capstone/retrieval_test/geogeorose_506.jpg'\n","#im='drive/My Drive/capstone/testing_retrieval/other_head21.jpg'\n","# a=Image.open(im)\n","# a=np.array(a)\n","# a = cv2.resize(a, (256, 256), interpolation=cv2.INTER_CUBIC)\n","\n","# print(a.shape)\n","# a=np.array(a)\n","# print(a.shape)\n","# a = cv2.cvtColor(np.array(a), cv2.COLOR_BGR2RGB)\n","\n","a=cv2.imread(im)\n","#a=cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n","r=ranking_module(a)\n","#print(r)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w98Uadgyn6S1","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}